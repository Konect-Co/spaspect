At the heart of SpaSpect technology lies a calculation of the aerial coordinates of people from the feed of a stationary camera.

How is this calculation achieved? To explain this, let's start by working backwards. Another way of representing a point in 3D space is the intersection of a line and a plane. In this case, the plane represents the floor, while the line represents the ray from the camera to a particular point on the floor, whether it be someone's foot or the base of a fire hydrant.

If we can represent both these objects in 3D space: 1) the ray from the camera to a point on the floor and 2) the plane representing the floor, then we can easily calculate the intersection of these two and find the point representing the point on the floor. 

Now, let's quickly discuss our representation of the aerial coordinate system, and the degrees of freedom our camera has. First, the translational degrees of freedom include left and right, forward and backward, and up and down. Respectively, we represent each of these as x, y, and z. The directions of right/left and forward/backward are absolute, while the direction of up/down depend on the calibration of the camera. right, forward and up directions are positive, while left, backward, and down are negative directions.

The rotational degrees of freedom include those with axes in the x direction, y direction, and z direction. The first of these is represented with verticalAngle, which has a value between 0 (indicating direction pointing toward the horizon) and 1 (indicating direction pointing directly downward). The second is represented with horizontalAngle. horizontalAngle is absolute with a value of 0, while verticalAngle and tiltAngle depend on the calibration of the camera. horizontalAngle increases in the clockwise direction as viewed from above, while tiltAngle increases in the clockwise direction.

The coordinate system we use is unitless, and can be measured in feet, meters, or any measurement such as Santrimeters (unit of measurement of 64 inches named after our team member Santript, who is 5'4"). However, one thing to keep in mind is that there must be consistency of these units for a particular webcam calibration. For example, if the webcam height is specified in meters, the calibration for aerial coordinates must also be in meters. Likewise the units for aerial coordinates predictions will also be in meters.

Now, let's go over the calculations of the line and plane discussed above, starting with the plane. The plane representing the floor is a constant value that is determined following calibration. For example, z(x, y) = 0 represents level ground while z(x, y) = 2y-4. represents a slope ground increasing upward in the direction of the camera. In this case, height of zero is at the line y=2, or 2 units.

However, the calculated value for the line is different for each identified person. To determine the direction of the line from the camera to the point on the floor, we use the pixel coordinates approximating the point where the person is standing on the floor. These pixel coordinates are approximated by taking the midpoint of the bottom two coordinates of the bounding box of a person, as predicted through an object detection deep learning model.

The last missing piece in our technology, then, is how the conversion happens between pixel and aerial coordinates. We already know the camera direction in aerial coordinates. We use the fact that it is characteristic of rectilinear cameras for the distance of the pixel coordinate from the center of the image to be proportional to the tangent of the angle from the camera direction. So, we calculate the horizontal and vertical delta angle values by taking the inverse tangent of the horizontal and vertical distance of the pxiel coordinates and multiplying this value by a calibration constant k.

k is determined during calibration by taking the tangent of the angle between the camera direction and the ray from the camera to the floor coordinate. One assumption we make is that the value of k is the same in both the vertical and horizontal directions. This assumption is broken if the feed from the camera is stretched.

Until this point, we have discussed how the calculations are carried for the line from the camera to a particular point on the floor of a webcam. We also know the value of the plane from the time of calibration. We compute the intersection of the line and plane in 3D space. to find the value. The aerial coordinate is determined by taking the first two values of the aerial coordinate. Calculating the distance between people is only the starting of this technology, which can be expanded to provide analytics on highly visited regions in a room, the degree to which social distancing guidelines, detection of individuals wearing masks, pose detection to determine how often people touch their face, and more features.

There are a few limitations of our technology that are worth discussion now. The first is the assumption that the webcam has a stationary configuration in 3D space (including all 6 degrees of freedom). This assumption is violated for webcams that rotate over time or experience disturbances. Second is the assumption that the calibration constant is the same in all directions, which does not hold true if the image is stretched. Lastly, we assume that the floor is visible from the webcam at all times and that the pixel coordinate corresponding to a person's feet can be inferred. So, this model is not very resistant to occlusion of a person by another person or object. Furthermore, if partial occlusion occurs, such as below an individual's waist, then the model will assume that the individual's feet are in the wrong place, and will thus make an incorrect prediction.

Few improvements we have upcoming in the future include:
- incorporation with depth estimation prediction methods (ex: )
- improve and integration with existing geographic information systems (GIS) data, such as that from Google Maps
- develop a more complex representation of the floor that accounts for distortions
